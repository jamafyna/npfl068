\documentclass[12pt, a4paper]{report}
\setlength\textwidth{160mm}
\setlength\textheight{247mm}
\setlength\oddsidemargin{0mm}
\setlength\evensidemargin{15mm}
\setlength\topmargin{0mm}
\setlength\headsep{0mm}
\setlength\headheight{0mm}
%%%%%%%%%%%
\usepackage[czech]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{bbold}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{indentfirst}    %%% zaveď odsazení 1. odstavce
\usepackage{multicol}
% %%%%%%%%%%%%%%
% %sets
% \def\eset{\emptyset}
% \def\seq{\subseteq}
 \def\N{\mathbb{N}}
 \def\Z{\mathbb{Z}}
\def\Q{\mathbb{Q}}
 \def\Lra{\Letfrightarrow}
% %logic
 \def\E{\exists}
 \def\F{\forall}
 \def\imp{\Rightarrow}
 \def\and{\wedge}
 \def\or{\vee}
 \def\eqv{\Leftrightarrow}
 \def\bimp{\Leftarrow}
\theoremstyle{remark}
\newtheorem{prikl}{Příklad}
\newcommand{\calP}[0]{
\mathcal{P}
}
\newcommand{\seq}[0]{
\subseteq
}

 \def\obrazek#1#2{
% 	\begin{figure}[h!]
 %		\setlength\fboxsep{0pt}
 %		\setlength\fboxrule{1.5pt}
% \begin{centerline}
%\begin{minipage}[0.45]
\begin{center}
 		\includegraphics[width=#2\textwidth]{#1}%[scale=0.5]%width=0.5\textwidth]
\end{center}
%\end{minipage}
 %               \end{centerline}
 %		%\caption{#2}
 %	\end{figure}
 }

\begin{document}
\section*{1. část úkolu - Best Friends}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent

%\includegraphics{img-cz-chars.png}
\section*{2. část úkolu - Word Classes}

K určení úplné hierarchie slov, resp. sloučení slov na 15 tříd, jsem použila algoritmus z přednášky. Místo maximalizace celé Mutual Information (MI) jsem použila minimalizaci ztráty MI oproti předchozímu kroku, tedy kostra algoritmu vypadala následovně: 

\begin{itemize}
\item[1.] Inicializace - každému slovu přiřadím svoji třídu
\item[2.] Dokud nedostaneš požadovaný počet tříd (s výskytem alespoň 10x), opakuj:

        \begin{itemize}
        \item[2.1.] Najdi třídy $d$, $e$ (z tříd s výskytem alespoň 10x), pro které je ztráta entropie minimální.
        \item[2.2] Sluč třídy $d$, $e$ do jedné.
        \end{itemize}
\end{itemize}

\subsection*{Implementace}
Hlavní otázkou bylo, jak vhodně reprezentovat třídy, aby bylo nalezení dvou tříd ke sloučení a zároveň jejich sloučení rozumně rychlé. Zjistila jsem, že ideálně by se mi hodila datová struktura, ve které bych měla zahešované/uložené bigramy tříd, která vrací v konstatním čase odpověď, kolik daných bigramů $(c_1,c_2)$ se v textu vyskytuje, a která vrátí v konstantním čase odkaz na všechny bigramy $(c1,*),(*,c1)$ (těch může být až lineárně). Zároveň tato struktura by se měla rozumně rychle aktualizovat (kvůli provádění merge).

Program jsem psala v jazyce Python3. Nechtěla jsem (nebavilo mě) přímo používat přesnou implementaci z přednášek, použila jsem tedy trochu jiné struktury, myšlenky ale zůstávají stejné. V programu jsem zejména využila hashtabulky vracející levé, resp. pravé, sousedy dané třídy a dvakrát CountTable uchovávající počet výskytů daných tříd, resp. bigramů tříd.  

Program se spouští příkazem {\bf python3 ass2\_23\_wordclasses.py TEXT.ptg~1~param}, se třemi parametry, kde TEXT.ptg znamená text, který chceme zpracovat, ve formátu .ptg; 1 označuje, že spouštíme variantu pro slova a ne tagy; param značí počet zbývajících tříd, tedy 1 pro úplnou hierarchii slov a 15 pro sloučení do 15 tříd. 
Výsledek poté vypíše do souboru results-23-*.txt, hvězdička nahrazuje dodané parametry. Kromě výsledné třídy/15 tříd vypisuje také průběh slučování, vč. minimal loss. Výslednou třídu pak ve formátu: {\it slova patřící do této včetně závorek značících postupné slučování --- nový řádek --- slova patřící do této třídy}.

\subsection*{CZ}
Při sloučení na 15 tříd mě zaujala třída skládající se ze spojek, tj. {\it  který, které, aby, ale, že}, obzvlášť že nejprve se sloučily třídy {\it který, které}. Tato třída vyšla hezky ukázkově. Naopak mě překvapilo, že čárka s tečkou zůstaly v samotných jednoprvkových třídách, automaticky bych čekala, že se sloučí, ale zde je možnost, že je rozdílnost dána velkým počátečním písmenem po tečce. Podobným překvapením je samostatná třída se spojkou {\it a}, zatímco spojka {\it i} byla sloučena poměrně brzy. Důvodem může být jiná četnost výskytu. Naopak očekávané mi přijdou pěkná průběžná sloučení tříd {\it byl + jsou, musí + bude, (za, u)+ do}.


\subsection*{EN}

Přijdou mi pěkná a výstižná sloučení tříd {\it may, cannot}, hned jako druhé slučování,  a dále  {\it must + can, is + are, if + when, most + many, would + will}.

Zároveň mi přišlo zajímavé, že slučování tříd poměrně pěkně zachovává slovní druhy, např. při slučování do 15 tříd je třída 2 složena  z podstatných jmen, veškerá přídavná jména jsou ve třídě 6, třída 8 je složena z předložek.

Celkově mi přišlo zajímavé, že slova s velkým počátečním písmenem na začátku mají tendenci slučovat se k sobě, což může být způsobeno téměř jistou tečkou (či jiným méně častým interpunkčním znaménkem) před těmito slovy.

I v anglickém textu zůstaly tečce a čárce jejich vlastní třídy při slučování do 15 tříd, což může být způsobeno tím, že kontext kolem nich je různorodý a v souvislosti s tím, že slova s velkým počátečním písmenem se shlukují často k sobě, a tedy není výhodné ani sloučit tečku a čárku do jedné třídy. Nevhodnost sloučení tečky a čárky potvrzuje i skutečnost, že se do společné třídy dostanou až při posledním slučování, tedy sloučením všech posledních dvou tříd do jedné. Tato skutečnost mě velmi překvapila, protože před výsledkami (a zamyšlením) bych intuitivně čekala, že se velmi brzy sloučí do jedné třídy.

\section*{Tag Classes}
Implementační část byla shodná s úlohou Word classes, akorát místo na slova se algoritmus pouštěl na tagy a místo na prvních 8000 slov se pouštěl na všechna data. Pro angličtinu algoritmus seběhl velmi rychle, neboť angličtina má menší počet různých tagů (anglický text obsahoval pouze 36 různých tagů). Český text obsahoval 677 různých tagů vyskytujících se alespoň pětkrát.

\subsection*{EN}
Očekávané sloučení je PRP\$ a DT, které proběhlo jako 10. merge, neboť přivlastňovací zájmena a DET se vyskytují ve stejném kontextu --- nejčastěji se použije přivlastňovací zájmeno, nebo DET a následuje nejčastěji podstatné jméno nebo přídavné jméno. 


Dále mě zaujala třída ((JJ+JJS)+CD), sloučení JJ+JJS bylo opět očekávané, neboť v kontextu se nejčastěji bude vyskytovat člen před a podstatné jméno za. CD se k nim připojilo, neboť mají opět podobný kontext, v jakém se vyskytují (např. the one/best of them, my best/two friends, atd.).

Další očekávané sloučení je NNS + NN, neboť podstatná jména jednotného a množného čísla se vyskytují v podobném kontextu, navíc angličtina nerozlišuje jiný tvar slovesa, příd. jména apod. dle čísla, s výjimkou třetí osoby u slovesa a přivlastňovacích zájmen. Přišlo mi zajímavé, že jakmile se sloučily třídy NNS + NN, sloučily se třídy VBZ + VBP a navíc minimal loss byla nižší než v předchozích 2 krocích, což svědčí o tom, že sloučení NNS + NN pomohlo. Tento výsledek je zároveň logický, neboť se částečně pomohl sjednotit kontext zleva --- tj. dogs(NNS) are(VBP) hungry vs. child(NN) is(VBZ) small. 

Na rozdíl od zpracování slov ve 2. části úkolu, při slučování tagů se k sobě dostala interpunkční znaménka, v kroce 21 se sloučily třídy \{{: , .}\}.
Naopak mi přišlo divné sloučení tříd NNPS + SYM, a to hned v druhém kroku, zde mi přišlo, že byl výsledek ovlivněn malou diverzitou členů těchto tříd (tag SYM měla pouze pravostranná závorka, NNPS bylo velmi časté u States (z United States)).

\subsection*{CZ}


\




\

\

\







\subsection*{Porovnání obou textů}
Anglický text má větší entropii než český, tedy jazykový model pro angličtinu by hůře předpovídal následující slovo než pro češtinu. Oba texty obsahují přibližně stejný počet slov, anglický text obsahuje 4.5 krát méně různých slov a dvakrát méně různých bigramů, vyšší počet různých slov pro češtinu je ovlivněn např. skloňováním. Vyšší entropie anglického textu může být způsobena právě menším počtem různých slov - kdyby na každé slovo připadal větší počet možných následníků. 

Čeština má nepatrně větší průměrnou délku slov, tedy při záměně znaků má větší šanci, že změna zasáhne více slov. Dále čeština obsahuje více různých znaků (117 oproti 74), tedy ještě snižuje šanci, že budou vznikat stejná nová slova.

\subsection*{Cvičení sloučení textů}
Označím $E$ entropii obou textů a $N_1,N_2$ počet slov původních textů. Předpokládám, že podmíněnou pravděpodobnost budu určovat podle vzorečku: $$p(w_i|w_{i-1})=c_2(w_{i-1},w_i)/c_1(w_{n})$$ a pravděpodobnost $p(w_{i-1},w_{i})$ podle vzorečku: $$p(w_{i-1},w_{i})=c(w_{i-1},w_{i})/(N-1).$$ Konce textu nebudu nijak označovat, tedy počet bigramů bude o 1 menší než počet slov. Pak platí, že 
$$ E_{new}= E(\frac{N_1-1}{N_1+N_2-1}+\frac{N_2-1}{N_1+N_2-1}))+\frac{1}{N_1+N_2-1}\log_2{\frac{1}{c(w_{2,1})}}. $$
Poslední člen značí nový bigram, který vznikl sloučením obou textů a je tedy unikátní. $c(w_n)$ značí počet výskytů posledního slova prvního textu v prvním textu. Daný výraz se dá upravit na
$$ E+\frac{E-\log_2(c(w_{n}))}{N_1+N_2-1}. $$
Tedy podmíněná entropie nového textu se bude od nové lišit o $\frac{E-\log_2(c(w_{n}))}{N_1+N_2-1}$, tedy závisí na četnosti posledního slova prvního textu. Pokud bude $\log_2(c(w_{n}))$ menší než $E$, entropie nového textu bude větší.

\subsection*{Poznámky}
Podrobné výsledky v souborech cz.txt, en.txt.
Výsledky vypisuje program first.py na standardní výstup, spouštět s parametrem --- zpracovávaný text.


\section*{2. část úkolu - Cross Entropy and Language Modeling}

Nejprve jsem určila parametry $l_0, l_1, l_2, l_3$ pomocí EM algoritmu, za požadovanou přesnost jsem zvolila 0.001. Em algoritmus potřeboval pro dosažení požadované přesnosti 6 iterací pro český text a 9 iterací pro anglický text.

Při určení vyhlazovacích parametrů z trénovacích dat vyšel třetí parametr (parametr odpovídající trigramům) 1 a zbylé 0. Což je očekávaný výsledek, jelikož algoritmus se snaží minimalizovat entropii nového pravděpodobnostního rozdělení, tato entropie však bude nejmenší, pokud nové pravděpodobnostní rozdělení bude odpovídat pravěpodobnostnímu rozdělení pro trigramy, tedy když $\lambda_3=1$. Z tohoto důvodu musíme na určení parametrů používat jiná data než trénovací.

\subsection*{Porovnání obou jazyků}
Pro český text se dává nejvyšší váha unigramům (0.44). Bigramům a uniformnímu rozdělení se přiřazuje podobná váha (0.25), nejhorší jsou trigramy, jejichž váha je téměř zanedbatelná. Zatímco anglický text preferuje bigramy (0.5), poté unigramy (0.26), a dále preferuje trigramy před uniformním rozdělením. Důvodem bude to, že čeština obsahuje daleko více různých bigramů než angličtina a tedy češtině se vyplatí mít model sestavený spíše z unigramů než z bigramů, angličtině naopak. Dále v angličtině se bude vyskytovat méně různých trigramů než v češtině.

Křížová entropie je v češtině vyšší než v angličtině, tedy model pro angličtinu bude přesnější než model pro češtinu. Z toho by se dalo říci, že angličtina je lépe předpovídatelná (nebo testovací data více odpovídala trénovacím a heldout datům). Toto potvrzuje i graf pokrytí (Coverage graph), který ukazuje, že téměř všechna slova z testovacích dat již byla viděna v~trénovacích datech, zatímco v češtině bylo viděno $80-87 \%$. Větší rozdíl je ještě u~bigramů, kdy v češtině je v~trénovacích datech pokryto cca $30-45 \%$ testovacích dat, zatímco v angličtině $70-80 \%$. Další výsledek potvrzující, že angličtina má méně různých trigramů, je cca $35-45\%$ pokrytí, zatímco u češtiny je již malé ($7-20\%$).

\subsection*{Poznámky}
Celkové výsledky jsou uloženy v souborech 2-en.txt, 2-cz.txt. Program s názvem second.py se spouští s parametrem --- vstupní text.

\end{document}
